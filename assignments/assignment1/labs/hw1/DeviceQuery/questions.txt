# Name: Alan Manuel Loreto Corn√≠dez
# Assignment: Homework 1
# Class: ECE 569 | High Performance Computing
# Professor: Dr. Ali Akoglu
# Date: January 23

"What is the name of the GPU device used on the Ocelote?"
    - Device 0 name: Tesla P100-PCIE-16GB

"What is the compute capability of this device?"
    - Computational Capabilities: 6.0

"What are the shared, constant and global memory sizes for this device?"
    - Maximum shared memory size per block: 49152
    - Maximum constant memory size: 65536
    - Maximum global memory size: 17071800320

"What is the maximum size of the block dimensions for this device?"
    - Maximum block dimensions: 1024 x 1024 x 64

"What is the compute capability of the NVIDIA Fermi architecture?"
    Compute Capability of the Fermi Architecture: 2.x.

"What are the maximum block dimensions for GPUs with 5.0 compute capability?",
    - Accoring to https://docs.nvidia.com/cuda/cuda-c-programming-guide/#features-and-technical-specifications-technical-specifications-per-compute-capability
    - The maximum block size for a GPU with compute capability 5.0 is: 1024 x 1024 x 64 (the same as the Tesla P100 GPU in our HOC system).

"Suppose you are launching a one dimensional grid and block. If the hardware's maximum grid dimension is 
65535 and the maximum block dimension is 512, what is the maximum number threads can be launched on the GPU?"
    - Threads = # of threads per block X # of blocks
    - # of blocks = grid size / block size = 65,535 / 512 = 128 blocks
    - Max Threads = # threads per block * 128
    * Notes: NVIDIA states that for current GPUs, 1024 is the maximum # of threads per block *
    - Plugging 1024 for the # of threads per block renders:
    - Max # Threads = 1024 * 128 = 2^10 * 2^7 = 2^17 = 131,072 threads.


"Under what conditions might a programmer choose not want to launch the maximum number of threads?"
    Conditions that may cause a programmer to not launch the maximum number of trheads on a GPU include:
    - The algorithm used may cause the programmer to prefer to use a number of threads different from the warp size.
    - The programmer may want to save thread usage to leave room for other applications to utilize the GPU.
    - The programmer may choose to preserve algorithmic complexity in exchange for slower performance.

"What can limit a program from launching the maximum number of threads on a GPU?"
    - According to the NVIDIA Cuda programming documentation: 'Threads can be inactive for a variety of reasons including having exited earlier than other threads of their warp, having taken a different branch path than the branch path currently executed by the warp, or being the last threads of a block whose number of threads is not a multiple of the warp size.'
    - Another reason that the maxium number of threads would not execute is because the programmer selects to run a number of threads that is not a multiple of the warp size (32 threads).


"What is shared memory?"
    - A memory that is visible to all other members of a thread block cluster.
    - Shared memory lifetime of one block is the same for all members of that block.

"What is global memory?"
    - Memory that is shared between all GPU kernels. 

"What is constant memory?"
    - Constant memory may be accessed by all child kernels from their parents.(Unlike Global and Shared Memory, those are private to a block.)
    - It cannot be modified, it must be initialized prior to program launch.

"What does warp size signify on a GPU?"
    A warp is a collection of 32 GPU threads that can execute instructions.
    'Half-warp = 16 threads'
    'Quarter-warp = 8 threads'

"Is double precision supported on GPUs with 1.3 compute capability?"
    - According to: https://en.wikipedia.org/wiki/CUDA#Data_types, double precision (64 floating point) has been supported since 1.3
    - Yes.


"What does compute capability mean?"	
    - Compute capability is a standard used to identify the hardware features that a GPU posesses.
